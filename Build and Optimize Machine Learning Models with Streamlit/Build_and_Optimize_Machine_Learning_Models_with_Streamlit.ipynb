{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca12abe-9d90-46c7-a40b-3631fe7e7665",
   "metadata": {
    "name": "md_title",
    "collapsed": false
   },
   "source": "# Build and Optimize a Machine Learning Models in Snowflake Notebooks with Streamlit\n\nIn this notebook, we'll build and optimize machine learning models. We'll also sprinkle in UI interactivity with Streamlit widgets to allow users to experiment and play with the parameters and settings.\n\n## Libraries used\n- `streamlit` - build the frontend UI\n- `pandas` - handle and wrangle data\n- `numpy` - numerical computing\n- `scikit-learn` - build machine learning models\n- `altair` - data visualization\n\n## Protocol\nHere's a breakdown of what we'll be doing:\n1. Load and prepare a dataset for modeling.\n2. Perform grid search hyperparameter optimization using the radial basis function (RBF) kernel with the support vector machine (SVM) algorithm.\n3. Visualize the hyperparameter optimization via a heatmap and line chart.\n"
  },
  {
   "cell_type": "markdown",
   "id": "cc43846f-0d71-40d4-9c6c-ebd7e81e4db4",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Build the ML Hyperparameter Optimization App using Streamlit"
  },
  {
   "cell_type": "code",
   "id": "59bf3b1e-92f9-4a24-919a-b7ea11f164b6",
   "metadata": {
    "language": "python",
    "name": "py_app",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_wine\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nst.title('ML Hyperparameter Optimization')\n\n# Load wine dataset\ndataset = load_wine()\nX = dataset.data\ny = dataset.target\nfeature_names = dataset.feature_names\n\n# Create DataFrame\ndf = pd.DataFrame(X, columns=feature_names)\ndf['target'] = y\n\n# Display dataset info using metrics\nst.header('üìñ Dataset Information')\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    st.metric(\"Number of features\", len(feature_names))\nwith col2:\n    st.metric(\"Number of classes\", len(dataset.target_names))\nwith col3:\n    st.metric(\"Number of samples\", len(y))\n\n# Display class names\nformatted_classes = \", \".join([f\"`{i+1}`\" for i in range(len(dataset.target_names))])\nst.write(f\"Classes: {formatted_classes}\")\n\n# Display sample of the data\nwith st.expander(\"üëÄ See the dataset\"):\n    st.write(df.head())\n\n# Model hyperparameters using powers of 2\nst.header('‚öôÔ∏è Hyperparameters')\n\n# Parameter range selection\nst.subheader(\"Parameter Ranges (in powers of 2)\")\ncol1, col2 = st.columns(2)\n\n# Create list of powers of 2\npowers = list(range(-10, 11, 2))\n\nwith col1:\n    C_power_range = st.select_slider(\n        'C (Regularization) range - powers of 2',\n        options=powers,\n        value=(-4, 4),\n        help='C = 2^value'\n    )\n    st.info(f'''\n    C range: $2^{{{C_power_range[0]}}}$ to $2^{{{C_power_range[1]}}}$\n    \n    {2**C_power_range[0]:.6f} to {2**C_power_range[1]:.6f}\n    ''')\n\nwith col2:\n    gamma_power_range = st.select_slider(\n        'Œ≥ range - powers of 2',\n        options=powers,\n        value=(-4, 4),\n        help='gamma = 2^value'\n    )\n    st.info(f'''\n    Œ≥ range: $2^{{{gamma_power_range[0]}}}$ to $2^{{{gamma_power_range[1]}}}$\n    \n    {2**gamma_power_range[0]:.6f} to {2**gamma_power_range[1]:.6f}\n    ''')\n\n# Step size selection\nst.subheader(\"Step Size for Grid Search\")\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    C_step = st.slider('C step size', 0.1, 2.0, 0.5, 0.1)\nwith col2:\n    gamma_step = st.slider('Gamma step size', 0.1, 2.0, 0.5, 0.1)\nwith col3:\n    test_size = st.slider('Test size', 0.1, 0.5, 0.2)\n\nst.divider()\n\n# Split and scale data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create parameter grid using powers of 2 with specified step sizes\ndef create_param_range(start_power, end_power, step):\n    powers = np.arange(start_power, end_power + step, step)\n    return np.power(2, powers)\n\nC_range = create_param_range(C_power_range[0], C_power_range[1], C_step)\ngamma_range = create_param_range(gamma_power_range[0], gamma_power_range[1], gamma_step)\n\n# Train model with GridSearchCV\nparam_grid = {\n    'C': C_range,\n    'gamma': gamma_range\n}\n\nsvm = SVC(kernel='rbf', random_state=42)\ngrid = GridSearchCV(svm, param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\n\n# Results\ny_pred = grid.predict(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Display metrics in columns\nmetrics1, metrics2, metrics3 = st.columns(3)\nwith metrics1:\n    st.header('Model Performance')\n    st.metric(\"Accuracy\", f\"{accuracy:.2f}\")\nwith metrics2:\n    best_C_power = np.log2(grid.best_params_['C'])\n    st.header('Best Parameters')\n    st.write(\"C\")\n    st.write(f\"$2^{{{best_C_power:.1f}}}$ = {grid.best_params_['C']:.6f}\")\n    st.write(f\"\")\nwith metrics3:\n    best_gamma_power = np.log2(grid.best_params_['gamma'])\n    st.header('Û†Ä†Û†Ä†‚Äé')\n    st.write(\"Œ≥\")\n    st.write(f\"$2^{{{best_gamma_power:.1f}}}$ = {grid.best_params_['gamma']:.6f}\")\n\n# Create visualization data with means and standard deviations\nresults = pd.DataFrame(grid.cv_results_)\nparam_results = pd.DataFrame({\n    'C': np.log2(results['param_C']),\n    'gamma': np.log2(results['param_gamma']),\n    'score': results['mean_test_score']\n})\n\n# Calculate means and standard errors for C\nC_stats = param_results.groupby('C').agg({\n    'score': ['mean', 'std', 'count']\n}).reset_index()\nC_stats.columns = ['C', 'mean_score', 'std_score', 'count']\nC_stats['stderr'] = C_stats['std_score'] / np.sqrt(C_stats['count'])\nC_stats['ci_upper'] = C_stats['mean_score'] + (2 * C_stats['stderr'])\nC_stats['ci_lower'] = C_stats['mean_score'] - (2 * C_stats['stderr'])\n\n# Calculate means and standard errors for gamma\ngamma_stats = param_results.groupby('gamma').agg({\n    'score': ['mean', 'std', 'count']\n}).reset_index()\ngamma_stats.columns = ['gamma', 'mean_score', 'std_score', 'count']\ngamma_stats['stderr'] = gamma_stats['std_score'] / np.sqrt(gamma_stats['count'])\ngamma_stats['ci_upper'] = gamma_stats['mean_score'] + (2 * gamma_stats['stderr'])\ngamma_stats['ci_lower'] = gamma_stats['mean_score'] - (2 * gamma_stats['stderr'])\n\n# Create heatmap\nst.header(\"Hyperparameter optimization\")\ncolor_schemes = ['yellowgreenblue', 'spectral', 'viridis', 'inferno', 'magma', 'plasma', 'turbo', 'greenblue', 'blues', 'reds', 'greens', 'purples', 'oranges']\nselected_color = st.selectbox('Select heatmap color scheme:', color_schemes)\n\n# Create heatmap with grid lines and selected color scheme\nheatmap = alt.Chart(param_results).mark_rect().encode(\n    x=alt.X('C:Q', \n            title='C parameter', \n            scale=alt.Scale(domain=[C_power_range[0], C_power_range[1]]),\n            axis=alt.Axis(grid=True, gridDash=[5,5])),\n    y=alt.Y('gamma:Q', \n            title='Œ≥ parameter', \n            scale=alt.Scale(domain=[gamma_power_range[0], gamma_power_range[1]]),\n            axis=alt.Axis(grid=True, gridDash=[5,5])),\n    color=alt.Color('score:Q', \n                   title='Cross-validation Score',\n                   scale=alt.Scale(scheme=selected_color)),\n    tooltip=['C', 'gamma', alt.Tooltip('score:Q', format='.3f')]\n).transform_window(\n    row_number='row_number()'\n).transform_fold(['score']\n).properties(\n    width=900,\n    height=300,\n)\n\n# Add grid lines as a separate layer\ngrid = alt.Chart(param_results).mark_rule(color='darkgray', strokeOpacity=0.2).encode(\n    x='C:Q'\n).properties(\n    width=900,\n    height=300\n) + alt.Chart(param_results).mark_rule(color='darkgray', strokeOpacity=0.2).encode(\n    y='gamma:Q'\n).properties(\n    width=900,\n    height=300\n)\n\n# Combine heatmap and grid\nfinal_heatmap = (heatmap + grid)\nst.altair_chart(final_heatmap)\n\n# Define common Y axis title\ny_axis_title = 'Cross-validation Score'\n\n# Create C parameter plot with error bands\nc_line_base = alt.Chart(C_stats)\n\nc_line = c_line_base.mark_line().encode(\n    x=alt.X('C:Q', title='C parameter', \n            scale=alt.Scale(domain=[C_power_range[0], C_power_range[1]])),\n    y=alt.Y('mean_score:Q', title=y_axis_title, scale=alt.Scale(zero=False))\n)\n\nc_points = c_line_base.mark_point(size=50).encode(\n    x='C:Q',\n    y=alt.Y('mean_score:Q', title=y_axis_title),\n    tooltip=[\n        alt.Tooltip('C:Q', title='C', format='.1f'),\n        alt.Tooltip('mean_score:Q', title='Mean Score', format='.3f'),\n        alt.Tooltip('std_score:Q', title='Std Dev', format='.3f')\n    ]\n)\n\nc_errorbars = c_line_base.mark_errorbar().encode(\n    x='C:Q',\n    y=alt.Y('ci_lower:Q', title=y_axis_title),\n    y2='ci_upper:Q'\n)\n\nc_band = c_line_base.mark_area(opacity=0.3).encode(\n    x='C:Q',\n    y=alt.Y('ci_lower:Q', title=y_axis_title),\n    y2='ci_upper:Q'\n)\n\nc_plot = (c_band + c_line + c_errorbars + c_points).properties(\n    width=400,\n    height=300,\n)\n\n# Create gamma parameter plot with error bands\ngamma_line_base = alt.Chart(gamma_stats)\n\ngamma_line = gamma_line_base.mark_line().encode(\n    x=alt.X('gamma:Q', title='Œ≥ parameter', \n            scale=alt.Scale(domain=[gamma_power_range[0], gamma_power_range[1]])),\n    y=alt.Y('mean_score:Q', title=y_axis_title, scale=alt.Scale(zero=False))\n)\n\ngamma_points = gamma_line_base.mark_point(size=50).encode(\n    x='gamma:Q',\n    y=alt.Y('mean_score:Q', title=y_axis_title),\n    tooltip=[\n        alt.Tooltip('gamma:Q', title='Gamma', format='.1f'),\n        alt.Tooltip('mean_score:Q', title='Mean Score', format='.3f'),\n        alt.Tooltip('std_score:Q', title='Std Dev', format='.3f')\n    ]\n)\n\ngamma_errorbars = gamma_line_base.mark_errorbar().encode(\n    x='gamma:Q',\n    y=alt.Y('ci_lower:Q', title=y_axis_title),\n    y2='ci_upper:Q'\n)\n\ngamma_band = gamma_line_base.mark_area(opacity=0.3).encode(\n    x='gamma:Q',\n    y=alt.Y('ci_lower:Q', title=y_axis_title),\n    y2='ci_upper:Q'\n)\n\ngamma_plot = (gamma_band + gamma_line + gamma_errorbars + gamma_points).properties(\n    width=400,\n    height=300,\n)\n\ncol = st.columns(2)\nwith col[0]:\n    st.altair_chart(c_plot)\nwith col[1]:\n    st.altair_chart(gamma_plot)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e59b550-b740-4c15-a23e-a510b85762ce",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "## Resources\n\n- An overview of [Snowflake Notebooks](https://www.snowflake.com/en/data-cloud/notebooks/) and its capabilities.\n- About [Snowflake Notebooks](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks) in the [Snowflake Documentation](https://docs.snowflake.com/).\n- Further information on the use of Streamlit can be found at the [Streamlit Docs](https://docs.streamlit.io/)."
  }
 ]
}